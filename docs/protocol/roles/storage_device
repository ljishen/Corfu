-------------------------------------------------------------------

- Projection p, initially empty but not NULL.

- HashMap<ulong, <ulong, bool>> map
  Keep the map from ela to <epa, isDeleted>.

- String addr_auxiliary, initially NULL
  The address of auxiliary which takes from the request from
  clients or other replicas when it is NULL.

- ulong end_of_cleaned, initially -1.
  The watermark before which no unwritten addresses exist.

===================================================================
Functions
-------------------------------------------------------------------

Interfaces

- onRead(offset, section)
- onWrite(offset, section, data, replica_idx, addr_auxiliary)
- onTrim(range, epoch, offset, type)
===================================================================

// @param offset         : The offset from which to read.
// @param section        : The associated section of the offset
//                         from client.
Operation onRead(offset, section) {
    l_section = p.sectionOf(offset)

    // We have an assumption that once the offset is even written,
    // the associated section exists in the projection of the
    // relevant storage devices.

    // A new storage device which is mapped to another section
    // takes over the old network addr.
    if (l_section == NULL)
        return <err_sealed, NULL>

    // A new storage device which took over the old network addr
    // now is assign to a section including the offset.
    if (l_section.epoch > section.epoch)
        return <err_sealed, diff(section, l_section)>

    // Now the client has the associated section no older than
    // the one held by the storage device.

    // For the epoch of the associated section of this offset:
    //
    // Case 1:
    //   The epoch of the section in the request is no less
    //   than the epoch of the section used during the write to
    //   this offset.
    //     The current read is valid because of the linearizable
    //     consistency property of the write operation.
    //
    // Case 2:
    //   The epoch of the section in the request is less than the
    //   epoch of the section used during the write to this offset.
    //     This case is impossible. Because the write to the offset
    //     will ensure the epoch of the associated section in all
    //     replicas within this section is at least the epoch of
    //     associated section in the write request. Therefore, the
    //     request should be filtered by the previous logic.

    if (l_section.epoch < section.epoch)
        broadcastReplace(l_section, section)

    if (map.contains(offset))
        if (map.get(offset).isDeleted)
            return <err_deleted, NULL>
    else
        if (offset <= end_of_cleaned)
            return <err_deleted, NULL>
        else
            return <err_unwritten, NULL>

    addr = map.get(offset).epa
    return <-1, read(addr)>
}


// 1. The storage devices do not need to preload all relevant
//    sections.
// 2. The storage device takes the section from the request if it
//    does not have one associated with the offset in the request.
//    In this case, this section could be NOT the latest, but since
//    the write operation is linearizable consistency, meaning any
//    successful writes under an old section (all replicas should
//    agree on this section) are preserved and visible to all
//    subsequent reads under later version of the associated
//    section.
// 3. Since the storage devices drive the migration of sections,
//    any one which has old section will eventually upgrade to the
//    latest ones.
// 4. If the storage device has the associated section newer than
//    the section in request, return its own section.
// 5. If the storage device has the associated section older than
/     the section in request, broadcast the replacement of its own
//    old section with the new section among the replicas within
//    the section BEFORE continuing the write. The consistency of
//    the associated section among the replicas within the section
//    is important because a client could use an old version of the
//    associated section to read data of an offset from a wrong
//    storage device in the latest associated section. For example,
//    a client could try to use an old version of section to read
//    data of the offset from F', but the data is written under a
//    newer version of section to F, where F' and F is in the same
//    section (hence F' and F both have the section in their
//    projection).
//
// @param offset         : The offset to which the data is written.
// @param section        : The associated section of the offset
//                         from client.
// @param data           : The data to write.
// @param replica_idx    : The index of the replica in the relevant
//                         extent.
// @param addr_auxiliary : The network address of the auxiliary.
Operation onWrite(offset, section, data, replica_idx,
                  addr_auxiliary) {
    section = p.sectionOf(offset)
    if (section == NULL)
        p.insert(section)

    <l_range, idx> = p.isOf(offset)
    if (l_range == NULL)
        p.add(range)
    else if (l_range.epoch > range.epoch or
             !l_range.extents[idx].replicas[replica_idx].enabled)
        return <err_sealed, l_range>

    // Only check the validity on the primary
    if (replica_idx == 0)
        if (map.containts(offset))
            if (map.get(offset).isDeleted)
                return <err_deleted>
            else
                return <err_written>
        else if (offset <= end_of_cleaned)
              return <err_deleted>


    <range, idx> = p.isOf(offset)

    // Replica the append request if I'm the primary
    if (replica_idx == 0)
        // Hold place for this offset to avoid taking by other clients
        map.put(offset, NULL)

        for (int i = 1; i < len(range.extents[idx].replicas); i++)
            r = range.extents[idx].replicas[i]
            async send (append: range, epoch, offset, data, i) to r.addr
                on timeout
                    send (report: 'WRITE', offset, r.id) to config.auxiliary_addr

                    // Remove the placeholder so that client can try again
                    map.remove(offset)

                    return <err_sealed>
                on receive <errno>
                    if (errno != -1)
                        // Remove the placeholder so that client can try again
                        map.remove(offset)

                        return <errno>

    addr = p.parse(offset, replica_idx)
    write(data, addr)
    map.put(offset, addr)

    return -1
}


Operation onTrim(range, epoch, offset, type, replica_idx) {
    errno = check(range, offset, replica_idx)
    if (errno != -1)
        return <errno>

    // Only check the validity on the primary
    if (replica_idx == 0)
        if (!map.containts(offset))
            if (offset <= end_of_cleaned)
                return <err_deleted>
            else
                return <err_unwritten>
        else if (map.get(offset).isDeleted)
            return <err_deleted>


}


// (Offload)
Operation ping() {

}


// This function performs the checking for the range of the offset.
// There are only two cases for such range from all storage device:
//
// Case 1:
// All the ranges have the same epoch.
// - If it is the latest epoch, then everything is good.
// - If they share an old epoch, the range also much be the same
//   since the same offset should be mapped to the same range in
//   the same epoch. Because the evolution of projection will
//   always preserve all written positions, it is safe to read
//   or write under an old epoch.
//
// Case 2:
// The ranges do not agree on one epoch.
// - If the primary has a range that is as new as other replicas,
//   then eventually they will update to the same range.
// - If the primary has a range that is older than some replicas,
//   the primary will return <err_sealed> to the client.
Operation check(range, offset, replica_idx) {
    <l_range, idx> = p.isOf(offset)
    if (l_range == NULL or l_range.epoch < range.epoch)
        replace(l_range, range)
        return -1
    else if (l_range.epoch > range.epoch)
        return <err_sealed>

    if (!l_range.extents[idx].replicas[replica_idx].enabled)
        return <err_sealed>
}


// Replace the old section with the new section.
//
// @param o_section: The old section.
// @param n_section: The new seciton.
Operation onReplace(o_section, n_section) {
    p.replace(o_section, n_section)
}


Operation broadcastReplace(o_section, n_section) {
    Set<String> addrs = n_section.devices()
    for (String addr : addrs)
        async send (replace: o_section, n_section) to addr
}

===================================================================