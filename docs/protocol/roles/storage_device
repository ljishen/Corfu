-------------------------------------------------------------------

- Config config
The configuration that stores the auxiliary_addr. This information
is updated by the client or other storage device.

- Projection p, initially empty but not NULL.

- HashMap<ulong, <ulong, bool>> map
Keep the map from ela to <epa, isDeleted>.

- ulong end_of_cleaned, initially -1.
The watermark before which no unwritten addresses exist.

===================================================================
Functions
-------------------------------------------------------------------

Interfaces

- onRead(offset, epoch)
- onAppend(range, epoch, offset, data)
- onTrim(range, epoch, offset, type)
===================================================================

Operation onRead(range, epoch, offset) {
    errno = check(range, offset)
    if (errno != -1)
        return <errno, NULL>

    if (map.contains(offset))
        if (map.get(offset).isDeleted)
            return <err_deleted, NULL>
    else
        if (offset <= end_of_cleaned)
            return <err_deleted, NULL>
        else
            return <err_unwritten, NULL>

    addr = map.get(offset).epa
    return <-1, read(addr)>
}


Operation onAppend(range, epoch, offset, data, replica_idx) {
    errno = check(range, offset)
    if (errno != -1)
        return <errno>

    // Only check the validity on the primary
    if (replica_idx == 0)
        if (map.containts(offset))
            if (map.get(offset).isDeleted)
                return <err_deleted>
            else
                return <err_written>
        else if (offset <= end_of_cleaned)
              return <err_deleted>


    <range, idx> = p.isOf(offset)

    // Replica the append request if I'm the primary
    if (replica_idx == 0)
        // Hold place for this offset to avoid taking by other clients
        map.put(offset, NULL)

        for (int i = 1; i < len(range.extents[idx].replicas); i++)
            r = range.extents[idx].replicas[i]
            async send (append: range, epoch, offset, data, i) to r.addr
                on timeout
                    send (report: 'WRITE', offset, r.id) to config.auxiliary_addr

                    // Remove the placeholder so that client can try again
                    map.remove(offset)

                    return <err_sealed>
                on receive <errno>
                    if (errno != -1)
                        // Remove the placeholder so that client can try again
                        map.remove(offset)

                        return <errno>

    addr = p.parse(offset, replica_idx)
    write(data, addr)
    map.put(offset, addr)

    return -1
}


Operation onTrim(range, epoch, offset, type, replica_idx) {
    errno = check(range, offset)
    if (errno != -1)
        return <errno>

    // Only check the validity on the primary
    if (replica_idx == 0)
        if (!map.containts(offset))
            if (offset <= end_of_cleaned)
                return <err_deleted>
            else
                return <err_unwritten>
        else if (map.get(offset).isDeleted)
            return <err_deleted>


}


// (Offload)
Operation ping() {

}


// This function performs the checking for the range of the offset.
// There are only two cases for such range from all storage device:
//
// Case 1:
// All the ranges have the same epoch.
// - If it is the latest epoch, then everything is good.
// - If they share an old epoch, the range also much be the same
//   since the same offset should be mapped to the same range in
//   the same epoch. Because the evolution of projection will
//   always preserve all written positions, it is safe to write
//   under an old epoch.
//
// Case 2:
// The ranges do not agree on one epoch.
// - If the primary has a range that is as new as other replicas,
//   then eventually they will update to the same range.
// - If the primary has a range that is older than some replicas,
//   the primary will return <err_sealed> to the client.
Operation check(range, offset) {
    l_range = p.rangeOf(offset)
    if (l_range == NULL or l_range.epoch < range.epoch)
        replace(l_range, range)
        return -1
    else if (l_range.epoch > range.epoch)
        return <err_sealed>
}


// Replace the o_range with the n_range.
// Note that n_range's epoch is bigger than the o_range's.
Operation replace(o_range, n_range) {
    if (o_range != NULL and o_range.epoch > n_range.epoch)
        throw error

    if (o_range != NULL)
        // This is the case where n_range is long and o_range is
        // short. Since the number of ranges maps to a storage
        // device is relatively small, even doing full search for
        // the out-of-date ranges is cheap.
        if (n_range.to - n_range.from > o_range.to - o.range.from)
            for (Range r : p.ranges)
                if (n_range.from <= r.from and n_range.to >= r.to)
                    p.remove(r)
        else
            // This is the case where n_range is short and o_range
            // is long. We can directly replace the long range
            // with the new short range.
            p.remove(o_range)

    config.auxiliary_addr = n_range.auxiliary
    p.add(n_range)
}

===================================================================